{"cells":[{"metadata":{"trusted":true,"_uuid":"69eb6853eb414ce8b1f35a1bf4466e7ed78dde92"},"cell_type":"code","source":"# I am starting with a baseline model.\n# Then I will use Keras Deep Learning Algorithms to create a simple NN Model.\n# Then I will use CNN and LSTM to create an optimum algorithm as an improvememt on the Simple NN Model.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Importing Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\nimport time\nfrom tqdm import tqdm\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Wordcloud\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n#sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\n\n#Keras\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D,CuDNNLSTM\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d366a7a38559f54a539811c461d57d30c7fa47c"},"cell_type":"code","source":"#Loading Data\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38ebb908553a47381b2e6266138a129caf35ae1c"},"cell_type":"code","source":"#Head of the Data\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e354d1756c784134a4d16c9dae7b4becf6678846"},"cell_type":"code","source":"#Head of the Target\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d2415b5dc270d4e3ad2d65c4b5fb2bdda26acac","scrolled":true},"cell_type":"code","source":"#Checking if the data contains any null values\ntrain_df.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e4c131592c0641ef3fbcabe766ce00e5b6e1da7"},"cell_type":"code","source":"#Checking the Shape of Training and Testing Data\nprint(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b73fda9e2f1e12901934dd4cef1e6a90ece8635"},"cell_type":"code","source":"#Splitting the Data into training and validation\nquestions = train_df['question_text']\ntarget = train_df['target']\n\nquestions_train, questions_val, target_train, target_val = train_test_split(questions, target, test_size=0.15, random_state=1000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3079988996dc01c2bc1d423e09514fea4a2a400f"},"cell_type":"code","source":"#Wordcloud of Questions just for fun\nplt.rcParams['figure.figsize']=(10.0,8.0)    #(6.0,4.0)\nplt.rcParams['font.size']=16                #10 \nplt.rcParams['savefig.dpi']=300             #72 \nplt.rcParams['figure.subplot.bottom']=.1 \n\n\nstopwords = set(STOPWORDS)\n#data = pd.read_csv(\"../input/most_backed.csv\")\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=100,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(questions))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df394e31311846f1e8fb41d4905eca7031871d54"},"cell_type":"code","source":"#Defining a Baseline model\nvectorizer = CountVectorizer()\nvectorizer.fit(questions_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a7a22370c9807e2e3c64be856587c980c454330"},"cell_type":"code","source":"X_train = vectorizer.transform(questions_train)\nX_val  = vectorizer.transform(questions_val)\nX_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b160e8527d71000c394ff37763e69aa4081c672"},"cell_type":"code","source":"#Logistic Regression - Baseline Model 1\nclassifier = LogisticRegression()\nclassifier.fit(X_train, target_train)\nscore = classifier.score(X_val, target_val)\n\nprint(\"Accuracy:\", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"946f0777541e54112c6fda49979d6fdd83a699c4"},"cell_type":"code","source":"#Classification Report for Logistic Regression\npredictions = classifier.predict(X_val)\nprint(classification_report(target_val,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adebf38c914e423ca8e1db74716f73a3edd85ebf"},"cell_type":"code","source":"#SVM Algorithm - Baseline 2\n\ntext_clf_svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=10, random_state=42)\ntext_clf_svm.fit(X_train, target_train)\npredicted_svm = text_clf_svm.predict(X_val)\nnp.mean(predicted_svm == target_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a2d379d01fc362966171c412b2a6e1634c49334"},"cell_type":"code","source":"#Classification Report for SVM\nprint(classification_report(target_val,predicted_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a4a579853f3a11b8e41554401a246cf8ddd0820"},"cell_type":"code","source":"#Tokenizing using Keras\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(questions_train)\n\nX_train = tokenizer.texts_to_sequences(questions_train)\nX_test = tokenizer.texts_to_sequences(questions_val)\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint(questions_train[2])\nprint(X_train[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69c066a4b3b13a99019b36e2010972f7fbb1dcbd"},"cell_type":"code","source":"for word in ['the', 'all', 'happy', 'sad']:\n    print('{}: {}'.format(word, tokenizer.word_index[word]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d7d7498aaa08191fb49726c2785d6b6a2ba50f1"},"cell_type":"code","source":"maxlen = 100\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\nprint(X_train[0, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4f41d0644f4b80b69fec9121f4e8436b976a29a","scrolled":true},"cell_type":"code","source":"#First Keras Model\ninput_dim = X_train.shape[1]  # Number of features\n\nmodel = Sequential()\nmodel.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4ef48e37c482638ebf038a8027dea789ea00e0a"},"cell_type":"code","source":"history = model.fit(X_train, target_train, epochs=2, verbose=False, validation_data=(X_test, target_val), batch_size=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9233921f6f427b282bbab61206ff5c87657d5443"},"cell_type":"code","source":"loss, accuracy = model.evaluate(X_train, target_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, target_val, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e75f8be7335461d6785e3dccd43c8c280dbc41d7"},"cell_type":"code","source":"#Checking F1 Score for First Keras Model\npred_paragram_target_val = model.predict([X_test], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(target_val, (pred_paragram_target_val>thresh).astype(int))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ff9f889ff2361365eaea6caf8405385d2412ccf"},"cell_type":"code","source":"plt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28740243cfaa05237ee2effaf6c126efb0d284c5"},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8e15636806c4c9d1666b63ca319ce87fabafc63"},"cell_type":"code","source":"#Lets use the pre-trained embeddings provided to us\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e135c09229b68cb3bfb6f7202bab8ea92cbb2950"},"cell_type":"markdown","source":"# Pretrained Embeddings Glove"},{"metadata":{"trusted":true,"_uuid":"d86c744c7c2f7121add124bd4bc17e0b5b0f2c42"},"cell_type":"code","source":"## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\nEMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbf3057fb19509579b1b3b04d001685b2cf47b75"},"cell_type":"code","source":"nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\nnonzero_elements / vocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb4bd5b467b4513134e5c101513c2c56c5a24a06"},"cell_type":"code","source":"#Keras Model With Pretrained Glove Text\n\"\"\"\nmodel = Sequential()\nmodel.add(layers.Embedding(max_features, embed_size, \n                           weights=[embedding_matrix], \n                           input_length=maxlen, \n                           trainable=False))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9811ea1697558cc908f95529c2cca6556882b519"},"cell_type":"code","source":"#Trainable = False\n\"\"\"\nhistory = model.fit(X_train, target_train,\n                    epochs=2,\n                    verbose=False,\n                    validation_data=(X_test, target_val),\n                    batch_size=800)\nloss, accuracy = model.evaluate(X_train, target_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, target_val, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0731bc27e39897c46b2e81c68e9fe585402d959"},"cell_type":"code","source":"#Trainable = True\nmodel = Sequential()\nmodel.add(layers.Embedding(max_features, embed_size, \n                           weights=[embedding_matrix], \n                           input_length=maxlen, \n                           trainable=True))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4ebe2ce1360ed076f23a6624e8ff988e20a06cc"},"cell_type":"code","source":"history = model.fit(X_train, target_train,\n                    epochs=2,\n                    verbose=False,\n                    validation_data=(X_test, target_val),\n                    batch_size=800)\nloss, accuracy = model.evaluate(X_train, target_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, target_val, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ffeb70f889f41864a8104dd666dbb4256e7f707"},"cell_type":"code","source":"#CNN\n\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(layers.Embedding(max_features, embedding_dim, input_length=maxlen))\nmodel.add(layers.Conv1D(128, 5, activation='relu'))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6917b66ae394bb5c7f553c369a6a9c1dfdc3fb3"},"cell_type":"code","source":"history = model.fit(X_train, target_train,\n                    epochs=2,\n                    verbose=False,\n                    validation_data=(X_test, target_val),\n                    batch_size=600)\nloss, accuracy = model.evaluate(X_train, target_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, target_val, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72db60640b28c60486cfdb287364fc38f12753f9"},"cell_type":"code","source":"#Checking F1 Score for CNN Model\npred_paragram_target_val = model.predict([X_test], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(target_val, (pred_paragram_target_val>thresh).astype(int))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"714f0ac786f53b7853e259d72d0bb8fba436ab25"},"cell_type":"code","source":"#Recurrent LSTM\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(layers.Embedding(max_features, embedding_dim, input_length=maxlen))\nmodel.add(Bidirectional(CuDNNLSTM(64, return_sequences = True)))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e21022a3d47cc21f9bb2d078baddcb514dfcfae8"},"cell_type":"code","source":"history = model.fit(X_train, target_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, target_val), batch_size=800)\nloss, accuracy = model.evaluate(X_train, target_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, target_val, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40a179c8202df5064fe35cfe55a96971b4562d0f"},"cell_type":"code","source":"#Checking F1 Score for CSTM Model with Epoch 10\npred_paragram_target_val = model.predict([X_test], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(target_val, (pred_paragram_target_val>thresh).astype(int))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2384cab386af2d8c8856835836bc336341f3732"},"cell_type":"code","source":"#For Test Dataset\nquestions_test = test_df['question_text']\nquestions_test[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46e940e3e691e1446da9057c235bfeaef9534dd2"},"cell_type":"code","source":"embed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(questions_test)\n\ntest = tokenizer.texts_to_sequences(questions_test)\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint(questions_test[2])\nprint(test[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cfe927852a0d35df744cd8f0c35adeb2aa0395b"},"cell_type":"code","source":"maxlen = 100\ntest = pad_sequences(test, padding='post', maxlen=maxlen)\nprint(test[0, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd0273d3c8c74b8b4d4149e5b214f9853e6f8838"},"cell_type":"code","source":"#Predict\npredicted_test_y = model.predict([test], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b5917e62a4d0576b262a6cded5c02370e0a5146"},"cell_type":"code","source":"#Final Output\npredicted_test_y = (predicted_test_y>0.33).astype(int)\noutput = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\noutput['prediction'] = predicted_test_y\noutput.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}